{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jhyJMxatuDF"
      },
      "outputs": [],
      "source": [
        "\n",
        "############################ EDA ########################################\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "import data_loader as dl\n",
        "import  quality_scorer as qs\n",
        "\n",
        "## load data into df\n",
        "listings = dl.load_data()\n",
        "print(f\"Loaded {len(listings):,} listings\")\n",
        "\n",
        "df = pd.DataFrame(listings)\n",
        "print(f\"data shape: {df.shape}\")\n",
        "print(f\"column names: {df.columns.tolist()}\")\n",
        "\n",
        "## create duckdb LINK\n",
        "conn = duckdb.connect()\n",
        "conn.register('listings', df)\n",
        "\n",
        "\n",
        "## sales distribution\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "SELECT\n",
        "    -- Basic statistics\n",
        "    MAX(sold_quantity) as max_sold,\n",
        "    MIN(sold_quantity) as min_sold,\n",
        "    AVG(sold_quantity) as avg_sold,\n",
        "    sum(sold_quantity) as total_sold,\n",
        "\n",
        "    -- Median (50th percentile)\n",
        "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY sold_quantity) as median_sold,\n",
        "\n",
        "    -- 25th and 75th percentiles (Q1 and Q3)\n",
        "    PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY sold_quantity) as p10_sold,\n",
        "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY sold_quantity) as p25_sold,\n",
        "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY sold_quantity) as p75_sold,\n",
        "    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY sold_quantity) as p90_sold,\n",
        "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY sold_quantity) as p95_sold,\n",
        "\n",
        "    -- Count statistics\n",
        "    COUNT(*) as total_listings,\n",
        "    COUNT(sold_quantity) as non_null_sold,\n",
        "\n",
        "FROM listings\n",
        "WHERE sold_quantity IS NOT NULL;\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## price distribution\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    SELECT\n",
        "         price/ base_price as price_change, count(1) FROM listings\n",
        "        group by 1\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    SELECT\n",
        "    MAX(price) as max_price,\n",
        "    MIN(price) as min_price,\n",
        "    AVG(price) as avg_price,\n",
        "   -- Median (50th percentile)\n",
        "    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY price) as median_price,\n",
        "\n",
        "    -- 25th and 75th percentiles (Q1 and Q3)\n",
        "    PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY price) as p10_price,\n",
        "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY price) as p25_price,\n",
        "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY price) as p75_price,\n",
        "    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY price) as p90_price,\n",
        "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY price) as p95_price,\t  FROM listings\n",
        "        where price\n",
        "\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## title analysis\n",
        "## get distribution\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    select count(distinct title) as distinct_title,\n",
        "    MAX(cnt) as max_cnt,\n",
        "    MIN(cnt) as min_cnt,\n",
        "    AVG(cnt) as avg_cnt,\n",
        "    MAX(length) as max_length,\n",
        "    MIN(length) as min_length,\n",
        "    AVG(length) as avg_length,\n",
        "    PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY length) as p10_length,\n",
        "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY length) as p25_length,\n",
        "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY length) as p75_length,\n",
        "    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY length) as p90_length,\n",
        "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY length) as p95_length,\n",
        "    from (SELECT\n",
        "        title,len(title) as length, count(1) as cnt  FROM listings\n",
        "        group by 1)\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## title length and sales correlation\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    with ranked_data as  (SELECT\n",
        "    title,\n",
        "    category_id,\n",
        "    length,\n",
        "    sold_quantity,\n",
        "    -- Length ranking within category\n",
        "    RANK() OVER (PARTITION BY category_id ORDER BY length DESC) as length_rank_in_category,\n",
        "    -- Sales ranking within category\n",
        "    RANK() OVER (PARTITION BY category_id ORDER BY sold_quantity DESC) as sales_rank_in_category\n",
        "FROM (\n",
        "    SELECT\n",
        "        title,\n",
        "        category_id,\n",
        "        LENGTH(title) as length,\n",
        "        SUM(sold_quantity) as sold_quantity,\n",
        "        -- Count titles per category for filtering\n",
        "        COUNT(DISTINCT title) OVER (PARTITION BY category_id) as titles_per_category\n",
        "    FROM listings\n",
        "    WHERE title IS NOT NULL\n",
        "      AND category_id IS NOT NULL\n",
        "      and sold_quantity>0\n",
        "    GROUP BY title, category_id\n",
        ") as title_stats\n",
        "WHERE titles_per_category >= 10  -- Keep only categories with 10+ distinct titles\n",
        "\n",
        ")\n",
        "SELECT\n",
        "    -- Check correlation\n",
        "    CORR(length_rank_in_category, sales_rank_in_category) as correlation_coefficient,\n",
        "    -- Compare top rankings\n",
        "    COUNT(CASE WHEN length_rank_in_category <= 10 AND sales_rank_in_category <= 10 THEN 1 END) as both_top_10,\n",
        "    COUNT(CASE WHEN length_rank_in_category > 10 AND sales_rank_in_category > 10 THEN 1 END) as both_bottom,\n",
        "    COUNT(CASE WHEN length_rank_in_category <= 10 AND sales_rank_in_category > 10 THEN 1 END) as long_title_low_sales,\n",
        "    COUNT(CASE WHEN length_rank_in_category > 10 AND sales_rank_in_category <= 10 THEN 1 END) as short_title_high_sales\n",
        "FROM ranked_data;\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## get distribution of category\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    SELECT\n",
        "        count(distinct category_id)  FROM listings\n",
        "\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## title quality analysis\n",
        "# Run the analysis\n",
        "df, filtered_df, correlation_results, performance_df = qs.analyze_title_quality_distribution_and_correlation(df)\n",
        "\n",
        "# Display key insights\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ”‘ KEY INSIGHTS FROM TITLE QUALITY ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get the correlation with title_score\n",
        "title_score_corr = next(r for r in correlation_results if r['metric'] == 'title_score')\n",
        "print(f\"\\n1. Overall Title Quality Correlation:\")\n",
        "print(f\"   â€¢ Correlation with sales rank: {title_score_corr['corr_with_sales_rank']:.3f}\")\n",
        "print(f\"     (Negative = better titles have better sales ranking)\")\n",
        "print(f\"   â€¢ Correlation with sold quantity: {title_score_corr['corr_with_sold_quantity']:.3f}\")\n",
        "print(f\"     (Positive = better titles sell more units)\")\n",
        "\n",
        "# Most impactful metric\n",
        "sorted_correlations = sorted(correlation_results, key=lambda x: abs(x['corr_with_sales_rank']), reverse=True)\n",
        "top_metric = sorted_correlations[0]\n",
        "print(f\"\\n2. Most Impactful Metric:\")\n",
        "print(f\"   â€¢ {top_metric['metric']}: correlation = {top_metric['corr_with_sales_rank']:.3f}\")\n",
        "print(f\"     (Strongest relationship with sales ranking)\")\n",
        "\n",
        "# Quality category performance\n",
        "if len(performance_df) >= 2:\n",
        "    best_category = performance_df.iloc[0]  # Lowest percentile rank\n",
        "    worst_category = performance_df.iloc[-1]  # Highest percentile rank\n",
        "\n",
        "    print(f\"\\n3. Performance Difference:\")\n",
        "    print(f\"   â€¢ {best_category['title_quality_category']}: avg rank = {best_category['avg_percentile_rank']:.1f}%\")\n",
        "    print(f\"   â€¢ {worst_category['title_quality_category']}: avg rank = {worst_category['avg_percentile_rank']:.1f}%\")\n",
        "    print(f\"   â€¢ Difference: {worst_category['avg_percentile_rank'] - best_category['avg_percentile_rank']:.1f}% points\")\n",
        "\n",
        "## video analysis\n",
        "## get distribution\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    SELECT\n",
        "        case when video_id is not null then 1 else 0 end as with_video, count(1)  FROM listings\n",
        "        group by 1\n",
        "\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## video and sales correlation\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    WITH video_impact AS (\n",
        "    SELECT\n",
        "        case when video_id is not null then 1 else 0 end as with_video,\n",
        "        category_id,\n",
        "        RANK() OVER (PARTITION BY category_id ORDER BY SUM(sold_quantity) DESC) as category_sales_rank,\n",
        "        COUNT(*) OVER (PARTITION BY category_id) as titles_in_category\n",
        "    FROM listings\n",
        "    WHERE title IS NOT NULL\n",
        "      AND category_id IS NOT NULL\n",
        "      AND sold_quantity >0\n",
        "    GROUP BY title, category_id, video_id\n",
        ")\n",
        "SELECT\n",
        "    with_video,\n",
        "    COUNT(*) as total_listings,\n",
        "    ROUND(AVG(category_sales_rank), 1) as average_rank,\n",
        "    ROUND(AVG(category_sales_rank * 100.0 / titles_in_category), 1) as average_percentile,\n",
        "    -- Percentage in top ranks\n",
        "    ROUND(SUM(CASE WHEN category_sales_rank <= 10 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as pct_top_10,\n",
        "    ROUND(SUM(CASE WHEN category_sales_rank <= titles_in_category * 0.25 THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as pct_top_25_percent\n",
        "FROM video_impact\n",
        "where titles_in_category >=10\n",
        "GROUP BY with_video\n",
        "ORDER BY average_percentile;\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## update frequency analysis\n",
        "## get distribution\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    select case when date_update_gap=0 then 'never update'\n",
        "        when date_update_gap between 1 and 60 then 'update within two months'\n",
        "        when date_update_gap>61  then 'update after two months'\n",
        "        else null end as date_update_gap_group,sum(cnt) from  (SELECT\n",
        "        cast(last_updated as date) - cast(date_created as date) as date_update_gap,\n",
        "\n",
        "         count(1) as cnt  FROM listings\n",
        "        group by 1\n",
        "        order by 1)\n",
        "        group by 1\n",
        "\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## update frequency and sales relationship\n",
        "basic_stats = conn.execute(\"\"\"\n",
        "    WITH listing_ranks AS (\n",
        "    SELECT\n",
        "        id,\n",
        "        title,\n",
        "        category_id,\n",
        "        sold_quantity,\n",
        "        date_created,\n",
        "        last_updated,\n",
        "        -- Calculate update gap in days\n",
        "        cast(last_updated as date) - cast(date_created as date) as update_gap_days,\n",
        "        -- Flag for never updated\n",
        "        CASE\n",
        "            WHEN cast(last_updated as date) - cast(date_created as date) =0 THEN 'Never Updated'\n",
        "            ELSE 'Updated'\n",
        "        END as update_status,\n",
        "        -- Sales rank within category\n",
        "        RANK() OVER (PARTITION BY category_id ORDER BY sold_quantity DESC) as sales_rank_in_category,\n",
        "        -- Count in category for percentile\n",
        "        COUNT(*) OVER (PARTITION BY category_id) as listings_in_category\n",
        "    FROM listings\n",
        "    WHERE category_id IS NOT NULL\n",
        "      AND sold_quantity >0\n",
        "      AND date_created IS NOT NULL\n",
        "      AND last_updated IS NOT NULL\n",
        "),\n",
        "rank_comparison AS (\n",
        "    SELECT\n",
        "        update_status,\n",
        "        COUNT(*) as listing_count,\n",
        "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage,\n",
        "        -- Sales performance metrics\n",
        "        AVG(sales_rank_in_category) as avg_sales_rank,\n",
        "        MEDIAN(sales_rank_in_category) as median_sales_rank,\n",
        "        -- Percentile rank (lower is better)\n",
        "        AVG(sales_rank_in_category * 100.0 / listings_in_category) as avg_percentile_rank,\n",
        "        -- Top performers\n",
        "        SUM(CASE WHEN sales_rank_in_category <= 10 THEN 1 ELSE 0 END) as top_10_count,\n",
        "        SUM(CASE WHEN sales_rank_in_category <= listings_in_category * 0.25 THEN 1 ELSE 0 END) as top_25_percent_count\n",
        "    FROM listing_ranks\n",
        "    where listings_in_category>=10\n",
        "    GROUP BY update_status\n",
        ")\n",
        "SELECT * FROM rank_comparison\n",
        "ORDER BY avg_sales_rank;\n",
        "\n",
        "\"\"\").fetchdf()\n",
        "print(basic_stats)\n",
        "\n",
        "## picture analysis\n",
        "counts, sizes, max_sizes = qs.safe_get_picture_info(df)\n",
        "\n",
        "df['picture_count'] = counts\n",
        "df['first_size'] = sizes\n",
        "df['first_max_size'] = max_sizes\n",
        "\n",
        "print(\"âœ… done!\")\n",
        "\n",
        "print(\"Picture count distribution:\")\n",
        "pic_count_dist = df['picture_count'].value_counts().sort_index()\n",
        "for count, freq in pic_count_dist.items():\n",
        "    print(f\"  {count} pictures: {freq} products ({(freq/len(df)*100):.1f}%)\")\n",
        "\n",
        "print(\"Picture size distribution:\")\n",
        "if df['first_size'].notna().any():\n",
        "    size_dist = df['first_size'].value_counts().head(10)  # top 10 most common sizes\n",
        "    for size, count in size_dist.items():\n",
        "        print(f\"  size {size}: {count} products ({(count/len(df)*100):.1f}%)\")\n",
        "else:\n",
        "    print(\"no info\")\n",
        "\n",
        "print(\"Max Size distribution:\")\n",
        "if df['first_max_size'].notna().any():\n",
        "    size_dist = df['first_max_size'].value_counts().head(10)  # top 10 most common sizes\n",
        "    for size, count in size_dist.items():\n",
        "        print(f\"  size {size}: {count} products ({(count/len(df)*100):.1f}%)\")\n",
        "else:\n",
        "    print(\"no info\")\n",
        "\n",
        "\n",
        "\n",
        "## attribute analysis\n",
        "entry_counts, empty_counts, total_counts = qs.safe_analyze_attributes(df)\n",
        "# Add results to DataFrame\n",
        "df['attr_entries'] = entry_counts\n",
        "df['attr_empty_fields'] = empty_counts\n",
        "df['attr_total_fields'] = total_counts\n",
        "\n",
        "df['attr_completeness_pct'] = df.apply(qs.calculate_completeness, axis=1)\n",
        "\n",
        "print(f\"\\nâœ… Successfully added columns:\")\n",
        "print(f\"   - attr_entries: Number of attribute entries\")\n",
        "print(f\"   - attr_empty_fields: Number of empty fields\")\n",
        "print(f\"   - attr_total_fields: Total number of fields\")\n",
        "print(f\"   - attr_completeness_pct: Field completeness percentage\")\n",
        "\n",
        "# Show basic statistics with distribution information\n",
        "print(f\"\\nðŸ“Š BASIC STATISTICS:\")\n",
        "print(f\"Total rows analyzed: {len(df):,}\")\n",
        "\n",
        "# 1. Distribution of entry counts - show each unique value\n",
        "print(f\"\\n1. ATTRIBUTE ENTRY COUNT DISTRIBUTION:\")\n",
        "print(\"Each entry count value and how many IDs have that value:\")\n",
        "\n",
        "# Get value counts for entry counts\n",
        "entry_value_counts = df['attr_entries'].value_counts().sort_index()\n",
        "\n",
        "# Show all unique values\n",
        "for entry_count, id_count in entry_value_counts.items():\n",
        "    percentage = (id_count / len(df)) * 100\n",
        "    print(f\"  {entry_count} entries: {id_count:,} IDs ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Unique entry count values: {len(entry_value_counts)}\")\n",
        "print(f\"  Most common: {entry_value_counts.index[0]} entries ({entry_value_counts.iloc[0]:,} IDs)\")\n",
        "print(f\"  IDs with 0 entries: {entry_value_counts.get(0, 0):,} IDs\")\n",
        "print(f\"  IDs with â‰¥1 entries: {len(df) - entry_value_counts.get(0, 0):,} IDs\")\n",
        "\n",
        "\n",
        "# Add formatted missing fields column\n",
        "filtered_df['missing_fields_format'] = filtered_df.apply(qs.format_missing_fields, axis=1)\n",
        "\n",
        "# Group by the formatted missing fields and count IDs\n",
        "missing_distribution = filtered_df['missing_fields_format'].value_counts()\n",
        "\n",
        "print(f\"\\nDistribution of missing fields (sorted by ID count, descending):\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Show all groups\n",
        "rank = 1\n",
        "for missing_format, id_count in missing_distribution.items():\n",
        "    percentage = (id_count / len(filtered_df)) * 100\n",
        "    print(f\"{rank:2d}. {missing_format:40s}: {id_count:6,} IDs ({percentage:5.1f}%)\")\n",
        "    rank += 1\n",
        "\n",
        "# Run the analysis\n",
        "filtered_df, results_df = qs.analyze_attribute_impact_on_sales(df)\n",
        "\n",
        "# Display results\n",
        "# Using pandas DataFrame display with formatting\n",
        "display_df = results_df.copy().sort_values('avg_percentile_rank')\n",
        "\n",
        "# Format columns\n",
        "display_df['product_count'] = display_df['product_count'].apply(lambda x: f\"{x:,}\")\n",
        "display_df['avg_sales_rank'] = display_df['avg_sales_rank'].apply(lambda x: f\"{x:.1f}\")\n",
        "display_df['avg_percentile_rank'] = display_df['avg_percentile_rank'].apply(lambda x: f\"{x:.1f}\")\n",
        "display_df['avg_sold_quantity'] = display_df['avg_sold_quantity'].apply(lambda x: f\"{x:.1f}\")\n",
        "display_df['pct_top_10'] = display_df['pct_top_10'].apply(lambda x: f\"{x:.1f}%\")\n",
        "display_df['pct_top_25_percent'] = display_df['pct_top_25_percent'].apply(lambda x: f\"{x:.1f}%\")\n",
        "display_df['percentage_of_total'] = display_df['percentage_of_total'].apply(lambda x: f\"{x:.2f}%\")\n",
        "\n",
        "# Rename columns for display\n",
        "display_df = display_df.rename(columns={\n",
        "    'entry_group': 'Entries',\n",
        "    'completeness_group': 'Completeness',\n",
        "    'product_count': 'Products',\n",
        "    'avg_sales_rank': 'Avg Rank',\n",
        "    'avg_percentile_rank': 'Avg %Rank',\n",
        "    'avg_sold_quantity': 'Avg Sold',\n",
        "    'pct_top_10': '% Top 10',\n",
        "    'pct_top_25_percent': '% Top 25%',\n",
        "    'percentage_of_total': '% of Total'\n",
        "})\n",
        "\n",
        "print(f\"\\nðŸ“Š ATTRIBUTE QUALITY IMPACT ON SALES PERFORMANCE\")\n",
        "print(\"=\"*120)\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "\n",
        "\n",
        "# Statistical correlation analysis\n",
        "print(f\"\\nðŸ“ˆ CORRELATION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate correlations\n",
        "correlations = filtered_df[['attr_entries', 'attr_completeness_pct', 'sales_rank']].corr()\n",
        "\n",
        "print(\"Correlation matrix (Pearson):\")\n",
        "print(\"(Note: Negative correlation with sales_rank/percentile_rank means better sales)\")\n",
        "print(correlations.round(3))\n",
        "\n",
        "# 2. Distribution of completeness percentages - only for IDs with 1 or 2 entries\n",
        "print(f\"\\n2. COMPLETENESS DISTRIBUTION (ONLY 1 OR 2 ENTRIES):\")\n",
        "\n",
        "# Filter for IDs with 1 or 2 entries\n",
        "filtered_df = df[df['attr_entries'].isin([1, 2])].copy()\n",
        "\n",
        "print(f\"Analyzing {len(filtered_df):,} IDs with 1 or 2 attribute entries\")\n",
        "print(f\"  IDs with 1 entry: {(filtered_df['attr_entries'] == 1).sum():,}\")\n",
        "print(f\"  IDs with 2 entries: {(filtered_df['attr_entries'] == 2).sum():,}\")\n",
        "\n",
        "## shipping analysis\n",
        "print(\"ðŸ” Analyzing shipping field...\")\n",
        "\n",
        "# Initialize lists for each field\n",
        "has_shipping_data = []\n",
        "local_pick_up_vals = []\n",
        "free_shipping_vals = []\n",
        "mode_vals = []\n",
        "dimensions_vals = []\n",
        "methods_counts = []\n",
        "tags_counts = []\n",
        "\n",
        "# Process each shipping value\n",
        "for i, shipping_value in enumerate(df['shipping']):\n",
        "    if i % 10000 == 0 and i > 0:\n",
        "        print(f\"  Processed {i:,} rows...\")\n",
        "\n",
        "    analysis = qs.analyze_shipping_field(shipping_value)\n",
        "\n",
        "    has_shipping_data.append(analysis['has_shipping_data'])\n",
        "    local_pick_up_vals.append(analysis['local_pick_up'])\n",
        "    free_shipping_vals.append(analysis['free_shipping'])\n",
        "    mode_vals.append(analysis['mode'])\n",
        "    dimensions_vals.append(analysis['dimensions'])\n",
        "    methods_counts.append(analysis['methods_count'])\n",
        "    tags_counts.append(analysis['tags_count'])\n",
        "\n",
        "# Add to DataFrame\n",
        "df['shipping_has_data'] = has_shipping_data\n",
        "df['shipping_local_pick_up'] = local_pick_up_vals\n",
        "df['shipping_free_shipping'] = free_shipping_vals\n",
        "df['shipping_mode'] = mode_vals\n",
        "df['shipping_dimensions'] = dimensions_vals\n",
        "df['shipping_methods_count'] = methods_counts\n",
        "df['shipping_tags_count'] = tags_counts\n",
        "\n",
        "print(f\"\\nâœ… Shipping analysis complete! Processed {len(df):,} rows\")\n",
        "\n",
        "# Display distribution for each field\n",
        "print(f\"\\nðŸ“Š SHIPPING FIELD DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Has shipping data\n",
        "print(f\"\\n1. HAS SHIPPING DATA:\")\n",
        "has_data_count = df['shipping_has_data'].sum()\n",
        "no_data_count = len(df) - has_data_count\n",
        "print(f\"   With shipping data: {has_data_count:,} IDs ({(has_data_count/len(df)*100):.1f}%)\")\n",
        "print(f\"   Without shipping data: {no_data_count:,} IDs ({(no_data_count/len(df)*100):.1f}%)\")\n",
        "\n",
        "# 2. Local pick-up distribution\n",
        "print(f\"\\n2. LOCAL PICK-UP DISTRIBUTION:\")\n",
        "if df['shipping_local_pick_up'].notna().any():\n",
        "    local_pickup_dist = df['shipping_local_pick_up'].value_counts(dropna=False)\n",
        "    for value, count in local_pickup_dist.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        value_str = \"True\" if value is True else \"False\" if value is False else \"None\"\n",
        "        print(f\"   {value_str}: {count:,} IDs ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"   No local_pick_up data\")\n",
        "\n",
        "# 3. Free shipping distribution\n",
        "print(f\"\\n3. FREE SHIPPING DISTRIBUTION:\")\n",
        "if df['shipping_free_shipping'].notna().any():\n",
        "    free_shipping_dist = df['shipping_free_shipping'].value_counts(dropna=False)\n",
        "    for value, count in free_shipping_dist.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        value_str = \"True\" if value is True else \"False\" if value is False else \"None\"\n",
        "        print(f\"   {value_str}: {count:,} IDs ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"   No free_shipping data\")\n",
        "\n",
        "# 4. Mode distribution\n",
        "print(f\"\\n4. SHIPPING MODE DISTRIBUTION:\")\n",
        "if df['shipping_mode'].notna().any():\n",
        "    mode_dist = df['shipping_mode'].value_counts(dropna=False)\n",
        "    for mode_value, count in mode_dist.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        mode_str = str(mode_value) if mode_value is not None else \"None\"\n",
        "        print(f\"   '{mode_str}': {count:,} IDs ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"   No mode data\")\n",
        "\n",
        "# 5. Dimensions distribution\n",
        "print(f\"\\n5. DIMENSIONS DISTRIBUTION:\")\n",
        "if df['shipping_dimensions'].notna().any():\n",
        "    dimensions_dist = df['shipping_dimensions'].value_counts(dropna=False)\n",
        "    # Show only top values if too many\n",
        "    if len(dimensions_dist) <= 10:\n",
        "        for dim_value, count in dimensions_dist.items():\n",
        "            percentage = (count / len(df)) * 100\n",
        "            dim_str = str(dim_value) if dim_value is not None else \"None\"\n",
        "            print(f\"   '{dim_str}': {count:,} IDs ({percentage:.1f}%)\")\n",
        "    else:\n",
        "        # Show top 10\n",
        "        print(f\"   (Showing top 10 out of {len(dimensions_dist)} unique values)\")\n",
        "        top_dimensions = dimensions_dist.head(10)\n",
        "        for dim_value, count in top_dimensions.items():\n",
        "            percentage = (count / len(df)) * 100\n",
        "            dim_str = str(dim_value) if dim_value is not None else \"None\"\n",
        "            print(f\"   '{dim_str}': {count:,} IDs ({percentage:.1f}%)\")\n",
        "        # Show None count separately\n",
        "        if None in dimensions_dist.index:\n",
        "            none_count = dimensions_dist[None]\n",
        "            print(f\"   'None': {none_count:,} IDs ({(none_count/len(df)*100):.1f}%)\")\n",
        "else:\n",
        "    print(\"   No dimensions data\")\n",
        "\n",
        "# 6. Methods count distribution\n",
        "print(f\"\\n6. SHIPPING METHODS COUNT DISTRIBUTION:\")\n",
        "if df['shipping_methods_count'].notna().any():\n",
        "    methods_dist = df['shipping_methods_count'].value_counts().sort_index()\n",
        "    for count_value, id_count in methods_dist.items():\n",
        "        percentage = (id_count / len(df)) * 100\n",
        "        print(f\"   {count_value} methods: {id_count:,} IDs ({percentage:.1f}%)\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n   Methods count summary:\")\n",
        "    print(f\"     Average methods: {df['shipping_methods_count'].mean():.2f}\")\n",
        "    print(f\"     Max methods: {df['shipping_methods_count'].max()}\")\n",
        "    print(f\"     IDs with 0 methods: {methods_dist.get(0, 0):,}\")\n",
        "else:\n",
        "    print(\"   No methods count data\")\n",
        "\n",
        "# 7. Tags count distribution\n",
        "print(f\"\\n7. SHIPPING TAGS COUNT DISTRIBUTION:\")\n",
        "if df['shipping_tags_count'].notna().any():\n",
        "    tags_dist = df['shipping_tags_count'].value_counts().sort_index()\n",
        "    for count_value, id_count in tags_dist.items():\n",
        "        percentage = (id_count / len(df)) * 100\n",
        "        print(f\"   {count_value} tags: {id_count:,} IDs ({percentage:.1f}%)\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n   Tags count summary:\")\n",
        "    print(f\"     Average tags: {df['shipping_tags_count'].mean():.2f}\")\n",
        "    print(f\"     Max tags: {df['shipping_tags_count'].max()}\")\n",
        "    print(f\"     IDs with 0 tags: {tags_dist.get(0, 0):,}\")\n",
        "else:\n",
        "    print(\"   No tags count data\")\n",
        "\n",
        "\n",
        "\n",
        "#### END OF ANALYSIS, SAVE FILE\n",
        "df.to_csv('processed_df.csv', index=False)\n"
      ]
    }
  ]
}
